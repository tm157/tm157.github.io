<!doctype html>
<html>
<head>

    <!-- Google Analytics -->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-131085468-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-131085468-1');
</script>


    

    <meta http-equiv="Content-type" content="text/html;charset=UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <meta name="description" content="Tanya Marwah is a graduate student at Robotics Institute, CMU.">
    <meta name="author" content="Tanya Marwah">
    <link rel="alternate" type="application/rss+xml"  href="http://mohitsharma0690.github.io/feed.xml" title="Mohit Sharma RSS Feed">

    <title>Tanya Marwah | Home</title>

    <link rel="stylesheet" media="all" href="/css/mine.css" />
    <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    

    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">

    <script type="text/javascript" src="//use.typekit.net/uzs7hgs.js"></script>
    <script type="text/javascript">try{Typekit.load();}catch(e){}</script>

    

    <!-- Any other scripts or anything that the page may want. -->
    

    <script type="text/javascript">
    init_mathjax = function() {
        if (window.MathJax) {
            // MathJax loaded
            MathJax.Hub.Config({
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
                },
                displayAlign: 'center', // Change this to 'center' to center equations.
                "HTML-CSS": {
                    styles: {'.MathJax_Display': {"margin": 0}}
                }
            });
            MathJax.Hub.Queue(["Typeset",MathJax.Hub]);
        }
    }
    init_mathjax();
    </script>
    <script type="text/javascript"
        src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>


</head>

<body lang="en">
    <div class="container">
        
            <div class="header">
                
                <div class="avatar"></div>
                <div class="site_title"><h1>Tanya Marwah</h1></div>
                
            </div>

            <div class="content">
                
                <p>I am a Masters student in <a href="https://ri.cmu.edu">Robotics Institute</a>, <a href="https://cmu.edu">CMU</a>, where I’m advised by <a href="http://www.cs.cmu.edu/~kkitani/">Dr. Kris Kitani</a>. While at CMU I have also been a recipient of the <a href="http://www.siebelscholars.com/">Siebel Scholarship, 2019</a>.
<br /><br />
My current research interests lie in Computer Vision, Reinforcement Learning and Machine Learning. I want to build robust visual representation systems that can learn with limited human supervision using interactions or unlabeled experience and generalize these representations across different tasks.
<br /> <br />
Previously, I completed my Bachelors with Honors in Electrical Engineering from <a href="https://www.iith.ac.in/">Indian Insitute of Technology, Hyderabad (IIT-H)</a>. During this time I was advised by <a href="https://www.iith.ac.in/~vineethnb/">Dr. Vineeth N. Balasubramanian</a>.
<br /><br />
In the past, I have also worked as a Research Intern at <a href="https://www.bosch.in/">Bosch AI</a>.</p>

<p>Here is my <a href="/files/CV/CV.pdf">CV</a>, <a href="https://github.com/tm157/">Github</a>, <a href="https://scholar.google.com/citations?user=PGkWaiYAAAAJ">Google Scholar</a> or <a href="mailto:tmarwah@cs.cmu.edu">email me</a>.</p>

<h2 id="publications">Publications</h2>

<div class="grid">

    <div class="unit four-of-five">
        <ul class="projects">
        
            
                

                <li>
                
                    <img src="/images/realanim/teaser.png" width="180px" />
                

                <div>
                <h4>
                
                    <span><a href="                                              /realanim                                      ">Learning Articulated Human Textures from Single Image</a></span>
                
                </h4>

                
                    <p>We present a methodology to infer a textured and artic-ulated 3D model of a 
person from a single image. We propose a two-stream approach that decouples 
geometry and texture inference, and combines the outputs ofthe two streams
using a differentiable renderer, which en-ables end-to-end self-supervised
learning.</p>

                
                
                    <ul>
  <li><a href="">New!!</a></li>
  <li><a href="https://drive.google.com/file/d/1xsHMyxmbbVRv4_1luE85JPqhViXz3jgh/view?usp=sharing">Video!!</a></li>
</ul>

                
                </div>
                </li>
            
        
            
                

                <li>
                
                    <img src="/images/sync_draw/results_2_224x192.jpg" width="180px" />
                

                <div>
                <h4>
                
                    <span><a href="                                              /sync-draw                                      ">Attentive Semantic Video Generation using Captions</a></span>
                
                </h4>

                
                    <p>We propose a novel approach to perform variable length semantic video
generation using short text in the form of captions. We adopt a new perspective
to video generation and combine both the long-term and short-term dependencies
between video frames. Using this we are able to generate a video incrementally.</p>

                
                
                    <ul>
  <li><a href="">ICCV ‘17</a></li>
  <li><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Marwah_Attentive_Semantic_Video_ICCV_2017_paper.pdf">PDF</a></li>
</ul>

                
                </div>
                </li>
            
        
            
                

                <li>
                
                    <img src="/images/sync_draw_recurrent/result_1_224x192.jpg" width="180px" />
                

                <div>
                <h4>
                
                    <span><a href="                                              /sync-draw-recurrent                                      ">Sync-DRAW: Automatic Video Generation using Deep Recurrent Attentive Architectures</a></span>
                
                </h4>

                
                    <p>We introduce Synchronized Deep Recurrent Attentive WRiter (Sync-Draw), a novel 
generative model for video generation. We introduce a novel way to combine a 
Variational Autoencoder (VAE) with a Recurrent Attention Mechanism, to create
temporally dependent sequence of frames, that are gradually generated over time.</p>

                
                
                    <ul>
  <li><a href="">ACM-MM ‘17 (Oral)</a></li>
  <li><a href="https://dl.acm.org/citation.cfm?id=3123309">PDF</a></li>
</ul>

                
                </div>
                </li>
            
        
            
        
            
        
            
        
            
        
            
                

                <li>
                
                    <img src="/images/diabetic-retinopathy/result_1_224x205.jpg" width="180px" />
                

                <div>
                <h4>
                
                    <span><a href="                                              /atrophy-analysis                                      ">On the Relevance of Very Deep Networks for Diabetic Retinopathy Diagnostics</a></span>
                
                </h4>

                
                    <p>Detection of Diabetic Retinopathy (DR) has been worked on for a long time, but
no commercially viable solutions that work for differ- ent populations exist
yet. In this work, we investigate the performance of Very Deep Networks for the
binary classification of fundus images provided by EyePACS as part of Kaggle’s
DR detection challenge.</p>

                
                
                    <ul>
  <li><a href="">Applications of Cognitive Computing Systems and IBM Watson ‘17</a></li>
  <li><a href="">Research Intern, BOSCH AI, 2016</a></li>
  <li><a href="https://link.springer.com/chapter/10.1007/978-981-10-6418-0_6">PDF</a></li>
</ul>

                
                </div>
                </li>
            
        
            
                

                <li>
                
                    <img src="/images/sentiment_analysis/result_1_224x192.png" width="180px" />
                

                <div>
                <h4>
                
                    <span><a href="                                              /sentiment-analysis                                      ">Sentiment dynamics in social media news channels</a></span>
                
                </h4>

                
                    
<p>We compare the sentiment of social media news posts of television, radio and
print media, to show the differences in the ways these channels cover the news.
We also analyze users’ reactions and opinion sentiment on news posts with
different sentiments. We perform our experiments on a dataset extracted from
Facebook Pages of five popular news channels.</p>

                
                
                    <ul>
  <li><a href="">Online Social Networks and Media, Journal Paper</a></li>
  <li><a href="https://www.sciencedirect.com/science/article/pii/S2468696418300260">PDF</a></li>
</ul>

                
                </div>
                </li>
            
        
        </ul>
    </div>

    <div class="unit four-of-five">
        <ul class="projects">
        
        </ul>
    </div>

</div>

<hr />

<h2 id="projects">Projects</h2>

<ul class="projects">

    
    

    
    

    
    

    
        
            <li>
            <img src="/images/culinary_skills/cucumber_cutting_256x144.png" width="180px" />
        
        <div>
        <span class="sans">
        
            <span><a href="/manipulation">Using Imitation Learning for Learning Culinary Skills</a></span>
        
        </span>
        <p>
        04 Dec 2018<br />
        In this project we explored learning culinary skills such as cutting,
pouring, drizzling using Learning from Demonstrations (LfD). For our 
low level control policy we use Dynamic Motor Primitives which allow us
to learn versatile skills from few demonstrations. We tested our approach
on a 7-DOF Franka Arm. <a href="https://drive.google.com/file/d/19C7qWeAIjeL3rySQkuCKjZxIwIfHBfzM/view">Video</a>

        </p>
        </div>
        </li>
    

    
        
            <li>
            <img src="/images/scene_generation/architecture_small.png" width="180px" />
        
        <div>
        <span class="sans">
        
            <span><a href="/scene-generation">Incremental Image Generation using Scene Graphs</a></span>
        
        </span>
        <p>
        01 Dec 2018<br />
        We propose a method that enables the underlying model to generate an image incrementally based on a sequence of graph of scene descriptions (scene-graphs). We propose a recurrent network architecture such that the cumulative image generated at any point in the sequential generation is consistent with the previously generated images. Our model utilizes Graph Convolutional Networks (GCN) to cater to variable size scene graphs along with GAN based image translation networks to generate realistic multi-object images with high amount of variability. We demonstrate our model’s capability to generate context preserving scene-graph based image sequence using multi-modal datasets such as Coco-Stuff which have multi-object images along with annotations describing the visual scene.

        </p>
        </div>
        </li>
    

    
        
            <li>
            <img src="/images/image-compositing-using-gan/image.png" width="180px" />
        
        <div>
        <span class="sans">
        
            <span><a href="/image-compositing-using-gan">RL Based Multi-Object Image Compositing</a></span>
        
        </span>
        <p>
        15 May 2018<br />
        We propose a generative adversarial approach to multi-object compositing
based on [7] where we introduce multiple discriminators
to handle the different distributions. We use reinforcement
learning to train an agent which treats the discriminators as
bandits and learns to choose the right discriminator to train
the generator. The generator learns to iteratively predict the
parameters of aatial transformer to warp the foreground
object onto the background. This ability allows the genera-
tor to generalize over different stages of multi-object com-
positing. We demonstrate our approach on CLEVR dataset
and show that our approach is giving promising results and
has the potential to generalize over more complicated visual
scenes.

        </p>
        </div>
        </li>
    

    
        
            <li>
            <img src="/images/task_based_language_grounding/architecture_256x136.jpg" width="180px" />
        
        <div>
        <span class="sans">
        
            <span><a href="/task-based-language-grounding">Improving Task-Oriented Language Grounding using Disentangled Representations</a></span>
        
        </span>
        <p>
        01 May 2018<br />
        In this work, we aim to learn a disentangled representation of the visual scene
using β-VAE, and then combine it with instruction-based text representation using
a soft-attention mechanism. This generates a representation of the 
“instruction-conditioned” visual scene which is robust to variations with respect
to objects, actions and attributes like shape and color. This representation is then
used to learn a policy using standard reinforcement learning methods to execute
the instruction in the given scene. 

        </p>
        </div>
        </li>
    

    
    

    
    

</ul>

<hr />

<h2 id="notes">Notes</h2>

<ul class="projects notes">

    
    

    
    

    
    

    
    

    
    

    
    

    
    

    
    

    
    

</ul>

<hr />

<h2 id="other-stuff">Other stuff</h2>

<!--<ul class="projects notes">-->

<li class="nothumb">
<a href="https://github.com/tm157/dotfiles/blob/master/colors/photon.vim">VIM colors</a>: I'm a stickler for color themes and vim. Given that no colorscheme met my requirements, I went ahead and made my own. Do try it out :). 
</li>
<li class="nothumb">
I love to read about technology, hardware and experiment with them. Thus, I've a slightly embarrasing number of different keyboards at home. However, I have finally converged to the Mx Master 2S as my mouse of choice. 
</li>

<!--</ul>-->


                
            </div>

            

            <hr />
            <div class="footer"> Updated 22 Dec 2018 </div>
        </div>
        
    </div>
</body>
</html>
