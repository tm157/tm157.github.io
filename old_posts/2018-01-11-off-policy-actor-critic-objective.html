<!doctype html>
<html>
<head>

    <!-- Google Analytics -->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-112819737-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-112819737-1');
</script>


    

    <meta http-equiv="Content-type" content="text/html;charset=UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <meta name="description" content="Tanya Marwah is a Robotics graduate student.">
    <meta name="author" content="Tanya Marwah">
    <link rel="alternate" type="application/rss+xml"  href="http://mohitsharma0690.github.io/feed.xml" title="Mohit Sharma RSS Feed">

    <title>Tanya Marwah | Objective function in Off-Policy Actor-Critic</title>

    <link rel="stylesheet" media="all" href="/css/mine.css" />
    <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    

    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">

    <script type="text/javascript" src="//use.typekit.net/uzs7hgs.js"></script>
    <script type="text/javascript">try{Typekit.load();}catch(e){}</script>

    

    <!-- Any other scripts or anything that the page may want. -->
    

    <script type="text/javascript">
    init_mathjax = function() {
        if (window.MathJax) {
            // MathJax loaded
            MathJax.Hub.Config({
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
                },
                displayAlign: 'center', // Change this to 'center' to center equations.
                "HTML-CSS": {
                    styles: {'.MathJax_Display': {"margin": 0}}
                }
            });
            MathJax.Hub.Queue(["Typeset",MathJax.Hub]);
        }
    }
    init_mathjax();
    </script>
    <script type="text/javascript"
        src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>


</head>

<body lang="en">
    <div class="container">
        
        <div class="smaller_container">
        
            <div class="header">
                
                    <p>
                        <a href="/" rel="author">
                            <i class="fa fa-th"></i>
                            Tanya Marwah
                        </a>
                    </p>
                    <h1><p>Objective function in Off-Policy Actor-Critic</p></h1>
                
            </div>

            <div class="content">
                
                <aside>
                    
                </aside>
                <p>Why is the objective in off-policy actor-critic optimizing for the value function? The transition dynamics are assumed to converge to a stationary distribution, $\mu^\pi(s)$.  Given this the objective is to maximize the expected return,
<script type="math/tex">J(\pi) = \sum_{s,a} \mu^\pi(s)\pi(a|s)r(s, a)</script>.</p>

<p>This makes sense, since under the limiting distribution we would like to optimize one step returns. Note, that this formulation is also followed in the first Sutton’s initial policy gradient paper [1].</p>

<p>However, in contrast in the off-policy actor-critic paper [2], the authors use a slightly different formulation i.e., equation (3) is <script type="math/tex">J(\pi) = \sum_{s,a}\mu^{b}(s)V^{\pi}(s)\</script>. By this, we want to optimize the value function under the limiting distribution. This seems a bit weird since the value function optimizes the [expected discounted] reward. But given that we are working under the limiting distribution it makes little sense.</p>

<p>As is done usually, why don’t we use the start state distribution in the optimization equation? When we do this instead we do arrive at a similar algorithm as derived in [2]. Notice that this would happen because we would be using importance sampling to trade off the difference between our sampling (behavior) policy and the learner’s policy.</p>

<p>We emailed Dr. Degris with our queries but never received a response. It would be useful to find some other resource that clears up our misunderstanding, if any.</p>

<p>P.S.: An artifact of this can be seen in the Deep Deterministic Policy Gradient paper as well. The two optimization problems i.e,. for the on-policy and off-policy cases are different but they do not clarify that the assumptions for each should be different.</p>

                
            </div>

            

            <hr />
            <div class="footer"> Updated 10 Dec 2018 </div>
        </div>
        
        </div>
        
    </div>
</body>
</html>
